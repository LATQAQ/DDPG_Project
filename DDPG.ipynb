{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 超参数\n",
    "train_eps = 1000\n",
    "test_eps = 20\n",
    "max_steps = 5000\n",
    "gamma = 0.99\n",
    "batch_size = 128\n",
    "device = torch.device('cuda')\n",
    "tau = 1e-2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Actor(nn.Module): #定义Actor网络\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(num_states, 32)\n",
    "        self.linear2 = nn.Linear(32, 64)\n",
    "        self.linear3 = nn.Linear(64, 128)\n",
    "        self.linear4 = nn.Linear(128, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = torch.tanh(self.linear4(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Critic(nn.Module): #定义Critic网络\n",
    "    def __init__(self, num_state_action, num_action_value = 1):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = nn.Linear(num_state_action, 64)\n",
    "        self.linear2 = nn.Linear(64, 128)\n",
    "        self.linear3 = nn.Linear(128, num_action_value)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # 按维数1拼接\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=self.capacity)\n",
    "    def push(self,transitions):\n",
    "        '''_summary_\n",
    "        Args:\n",
    "            trainsitions (tuple): _description_\n",
    "        '''\n",
    "        self.buffer.append(transitions)\n",
    "    def sample(self, batch_size: int, sequential: bool = False):\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "        if sequential: # sequential sampling\n",
    "            rand = random.randint(0, len(self.buffer) - batch_size)\n",
    "            batch = [self.buffer[i] for i in range(rand, rand + batch_size)]\n",
    "            return zip(*batch)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "            return zip(*batch)\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, device, action_space, state_space, batch_size, gamma, tau):\n",
    "        self.device = device\n",
    "        self.critic = Critic(action_space+state_space,1).to(device)\n",
    "        self.actor = Actor(state_space,action_space).to(device)\n",
    "        self.target_critic = Critic(action_space+state_space,1).to(device)\n",
    "        self.target_actor = Actor(state_space,action_space).to(device)\n",
    "\n",
    "        # 复制参数到目标网络\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
    "        self.memory = ReplayBuffer(capacity= 100000)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        action = self.actor(state)\n",
    "        return action.detach().cpu().numpy()[0,0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        action = self.actor(state)\n",
    "        return action.cpu().numpy()[0,0]\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.memory) < self.batch_size:  # 当memory中不满足一个批量时，不更新策略\n",
    "            return\n",
    "        # 从经验回放中中随机采样一个批量的transition\n",
    "        state, action, reward, next_state, done = self.memory.sample(self.batch_size)\n",
    "        # 转变为张量\n",
    "        state = torch.FloatTensor(np.array(state)).to(self.device)\n",
    "        next_state = torch.FloatTensor(np.array(next_state)).to(self.device)\n",
    "        action = torch.FloatTensor(np.array(action)).to(self.device)\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1).to(self.device)\n",
    "        done = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # 计算actor_loss\n",
    "        actor_loss = self.critic(state, self.actor(state))\n",
    "        actor_loss = - actor_loss.mean()\n",
    "\n",
    "        # 计算下一时刻的预测动作价值\n",
    "        next_action = self.target_actor(next_state)\n",
    "        target_value = self.target_critic(next_state, next_action.detach())\n",
    "\n",
    "        # 计算y_t\n",
    "        expected_value = reward + (1.0 - done) * self.gamma * target_value\n",
    "        expected_value = torch.clamp(expected_value, -np.inf, np.inf)\n",
    "\n",
    "        # 计算critic_loss\n",
    "        actual_value = self.critic(state, action)\n",
    "        critic_loss = nn.MSELoss()(actual_value, expected_value.detach())\n",
    "\n",
    "        # 反向传播\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # 软更新\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) +\n",
    "                param.data * self.tau\n",
    "            )\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) +\n",
    "                param.data * self.tau\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class OUNoise(object): # Ornstein–Uhlenbeck噪声\n",
    "    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
    "        self.mu           = mu # OU噪声的参数\n",
    "        self.theta        = theta # OU噪声的参数\n",
    "        self.sigma        = max_sigma # OU噪声的参数\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.n_actions   = action_space.shape[0]\n",
    "        self.low          = action_space.low\n",
    "        self.high         = action_space.high\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.obs = np.ones(self.n_actions) * self.mu\n",
    "    def evolve_obs(self):\n",
    "        x  = self.obs\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.n_actions)\n",
    "        self.obs = x + dx\n",
    "        return self.obs\n",
    "    def get_action(self, action, t=0):\n",
    "        ou_obs = self.evolve_obs()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period) # sigma会逐渐衰减\n",
    "        return np.clip(action + ou_obs, self.low, self.high) # 动作加上噪声后进行剪切"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train(env, agent, train_eps, max_steps):\n",
    "    print(\"Start Training\")\n",
    "    ou_noise = OUNoise(env.action_space)  # 动作噪声\n",
    "    rewards = [] # 记录所有回合的奖励\n",
    "    for episode in range(train_eps):\n",
    "        state = env.reset()\n",
    "        ou_noise.reset()\n",
    "        ep_reward = 0\n",
    "        for step in range(max_steps):\n",
    "            action = agent.sample_action(state)\n",
    "            action = ou_noise.get_action(action, step+1)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            agent.memory.push((state, action, reward, next_state, done))\n",
    "            agent.update()\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if (episode+1)%10 == 0:\n",
    "            print(f\"Episode：{episode+1}/{train_eps}，Reward：{ep_reward:.2f}\")\n",
    "        rewards.append(ep_reward)\n",
    "    print(\"Training Complete\")\n",
    "    return rewards"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def test(env, agent, test_eps, max_steps):\n",
    "    print(\"Start Testing\")\n",
    "    rewards = [] # 记录所有回合的奖励\n",
    "    for episode in range(test_eps):\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "        for step in range(max_steps):\n",
    "            action = agent.predict_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "        print(f\"Episode：{episode+1}/{test_eps}，Reward：{ep_reward:.2f}\")\n",
    "    print(\"Testing Complete\")\n",
    "    return rewards"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def draw(rewards,tag):\n",
    "    sns.set(style='whitegrid')\n",
    "    fig = sns.relplot(y= rewards, kind= 'line', tag=tag)\n",
    "    plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "    ''' 将action范围重定在[0.1]之间\n",
    "    '''\n",
    "    def action(self, action):\n",
    "        low_bound   = self.action_space.low\n",
    "        upper_bound = self.action_space.high\n",
    "        action = low_bound + (action + 1.0) * 0.5 * (upper_bound - low_bound)\n",
    "        action = np.clip(action, low_bound, upper_bound)\n",
    "        return action\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        low_bound   = self.action_space.low\n",
    "        upper_bound = self.action_space.high\n",
    "        action = 2 * (action - low_bound) / (upper_bound - low_bound) - 1\n",
    "        action = np.clip(action, low_bound, upper_bound)\n",
    "        return action"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15163\\miniconda3\\envs\\PytorchL\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "C:\\Users\\15163\\miniconda3\\envs\\PytorchL\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Episode：10/1000，Reward：-1495.62\n",
      "Episode：20/1000，Reward：-996.47\n",
      "Episode：30/1000，Reward：-691.44\n",
      "Episode：40/1000，Reward：-336.38\n",
      "Episode：50/1000，Reward：-372.52\n",
      "Episode：60/1000，Reward：-520.83\n",
      "Episode：70/1000，Reward：-254.15\n",
      "Episode：80/1000，Reward：-250.65\n",
      "Episode：90/1000，Reward：-381.24\n",
      "Episode：100/1000，Reward：-494.17\n",
      "Episode：110/1000，Reward：-257.34\n",
      "Episode：120/1000，Reward：-500.12\n",
      "Episode：130/1000，Reward：-492.82\n",
      "Episode：140/1000，Reward：-377.74\n",
      "Episode：150/1000，Reward：-571.21\n",
      "Episode：160/1000，Reward：-620.47\n",
      "Episode：170/1000，Reward：-607.83\n",
      "Episode：180/1000，Reward：-454.11\n",
      "Episode：190/1000，Reward：-622.73\n",
      "Episode：200/1000，Reward：-499.55\n",
      "Episode：210/1000，Reward：-366.48\n",
      "Episode：220/1000，Reward：-372.88\n",
      "Episode：230/1000，Reward：-148.26\n",
      "Episode：240/1000，Reward：-737.94\n",
      "Episode：250/1000，Reward：-498.82\n",
      "Episode：260/1000，Reward：-729.78\n",
      "Episode：270/1000，Reward：-259.22\n",
      "Episode：280/1000，Reward：-399.05\n",
      "Episode：290/1000，Reward：-488.63\n",
      "Episode：300/1000，Reward：-333.24\n",
      "Episode：310/1000，Reward：-609.75\n",
      "Episode：320/1000，Reward：-359.98\n",
      "Episode：330/1000，Reward：-379.09\n",
      "Episode：340/1000，Reward：-502.74\n",
      "Episode：350/1000，Reward：-503.83\n",
      "Episode：360/1000，Reward：-433.86\n",
      "Episode：370/1000，Reward：-779.00\n",
      "Episode：380/1000，Reward：-384.16\n",
      "Episode：390/1000，Reward：-411.02\n",
      "Episode：400/1000，Reward：-615.84\n",
      "Episode：410/1000，Reward：-612.19\n",
      "Episode：420/1000，Reward：-495.64\n",
      "Episode：430/1000，Reward：-253.75\n",
      "Episode：440/1000，Reward：-478.83\n",
      "Episode：450/1000，Reward：-250.06\n",
      "Episode：460/1000，Reward：-382.33\n",
      "Episode：470/1000，Reward：-505.76\n",
      "Episode：480/1000，Reward：-368.34\n",
      "Episode：490/1000，Reward：-379.64\n",
      "Episode：500/1000，Reward：-131.95\n",
      "Episode：510/1000，Reward：-724.80\n",
      "Episode：520/1000，Reward：-488.66\n",
      "Episode：530/1000，Reward：-503.94\n",
      "Episode：540/1000，Reward：-508.76\n",
      "Episode：550/1000，Reward：-465.32\n",
      "Episode：560/1000，Reward：-404.89\n",
      "Episode：570/1000，Reward：-503.63\n",
      "Episode：580/1000，Reward：-621.88\n",
      "Episode：590/1000，Reward：-648.16\n",
      "Episode：600/1000，Reward：-492.60\n",
      "Episode：610/1000，Reward：-500.94\n",
      "Episode：620/1000，Reward：-508.64\n",
      "Episode：630/1000，Reward：-608.66\n",
      "Episode：640/1000，Reward：-590.07\n",
      "Episode：650/1000，Reward：-840.33\n",
      "Episode：660/1000，Reward：-496.13\n",
      "Episode：670/1000，Reward：-583.68\n",
      "Episode：680/1000，Reward：-610.56\n",
      "Episode：690/1000，Reward：-625.45\n",
      "Episode：700/1000，Reward：-495.21\n",
      "Episode：710/1000，Reward：-374.57\n",
      "Episode：720/1000，Reward：-624.62\n",
      "Episode：730/1000，Reward：-583.61\n",
      "Episode：740/1000，Reward：-627.94\n",
      "Episode：750/1000，Reward：-613.22\n",
      "Episode：760/1000，Reward：-520.95\n",
      "Episode：770/1000，Reward：-372.74\n",
      "Episode：780/1000，Reward：-624.17\n",
      "Episode：790/1000，Reward：-730.49\n",
      "Episode：800/1000，Reward：-583.57\n",
      "Episode：810/1000，Reward：-634.74\n",
      "Episode：820/1000，Reward：-618.72\n",
      "Episode：830/1000，Reward：-256.58\n",
      "Episode：840/1000，Reward：-731.20\n",
      "Episode：850/1000，Reward：-515.25\n",
      "Episode：860/1000，Reward：-384.79\n",
      "Episode：870/1000，Reward：-373.80\n",
      "Episode：880/1000，Reward：-498.40\n",
      "Episode：890/1000，Reward：-244.26\n",
      "Episode：900/1000，Reward：-643.07\n",
      "Episode：910/1000，Reward：-238.51\n",
      "Episode：920/1000，Reward：-374.67\n",
      "Episode：930/1000，Reward：-621.78\n",
      "Episode：940/1000，Reward：-360.27\n",
      "Episode：950/1000，Reward：-369.92\n",
      "Episode：960/1000，Reward：-253.98\n",
      "Episode：970/1000，Reward：-609.74\n",
      "Episode：980/1000，Reward：-244.65\n",
      "Episode：990/1000，Reward：-437.91\n",
      "Episode：1000/1000，Reward：-358.13\n",
      "Training Complete\n",
      "Start Testing\n",
      "Episode：1/20，Reward：-124.38\n",
      "Episode：2/20，Reward：-121.49\n",
      "Episode：3/20，Reward：-253.45\n",
      "Episode：4/20，Reward：-310.43\n",
      "Episode：5/20，Reward：-127.62\n",
      "Episode：6/20，Reward：-121.84\n",
      "Episode：7/20，Reward：-121.79\n",
      "Episode：8/20，Reward：-135.61\n",
      "Episode：9/20，Reward：-6.79\n",
      "Episode：10/20，Reward：-123.20\n",
      "Episode：11/20，Reward：-130.04\n",
      "Episode：12/20，Reward：-243.51\n",
      "Episode：13/20，Reward：-123.62\n",
      "Episode：14/20，Reward：-238.90\n",
      "Episode：15/20，Reward：-251.07\n",
      "Episode：16/20，Reward：-123.87\n",
      "Episode：17/20，Reward：-120.79\n",
      "Episode：18/20，Reward：-241.17\n",
      "Episode：19/20，Reward：-4.69\n",
      "Episode：20/20，Reward：-128.80\n",
      "Testing Complete\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'x'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\miniconda3\\envs\\PytorchL\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3080\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3079\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3080\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3081\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mpandas\\_libs\\index.pyx:70\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\index.pyx:101\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:4554\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:4562\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'x'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m test_res \u001B[38;5;241m=\u001B[39m test(env,agent,test_eps,max_steps)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# 画出结果\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_res\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtag\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m draw(test_res,tag\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[10], line 3\u001B[0m, in \u001B[0;36mdraw\u001B[1;34m(rewards, tag)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdraw\u001B[39m(rewards,tag):\n\u001B[0;32m      2\u001B[0m     sns\u001B[38;5;241m.\u001B[39mset(style\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwhitegrid\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m     fig \u001B[38;5;241m=\u001B[39m \u001B[43msns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mrewards\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkind\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mline\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtag\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtag\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m     plt\u001B[38;5;241m.\u001B[39mlegend()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PytorchL\\lib\\site-packages\\seaborn\\relational.py:955\u001B[0m, in \u001B[0;36mrelplot\u001B[1;34m(data, x, y, hue, size, style, units, row, col, col_wrap, row_order, col_order, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, dashes, style_order, legend, kind, height, aspect, facet_kws, **kwargs)\u001B[0m\n\u001B[0;32m    946\u001B[0m g \u001B[38;5;241m=\u001B[39m FacetGrid(\n\u001B[0;32m    947\u001B[0m     data\u001B[38;5;241m=\u001B[39mfull_data\u001B[38;5;241m.\u001B[39mdropna(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    948\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mgrid_kws,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    951\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfacet_kws\n\u001B[0;32m    952\u001B[0m )\n\u001B[0;32m    954\u001B[0m \u001B[38;5;66;03m# Draw the plot\u001B[39;00m\n\u001B[1;32m--> 955\u001B[0m g\u001B[38;5;241m.\u001B[39mmap_dataframe(func, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mplot_kws)\n\u001B[0;32m    957\u001B[0m \u001B[38;5;66;03m# Label the axes, using the original variables\u001B[39;00m\n\u001B[0;32m    958\u001B[0m \u001B[38;5;66;03m# Pass \"\" when the variable name is None to overwrite internal variables\u001B[39;00m\n\u001B[0;32m    959\u001B[0m g\u001B[38;5;241m.\u001B[39mset_axis_labels(variables\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m, variables\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PytorchL\\lib\\site-packages\\seaborn\\axisgrid.py:819\u001B[0m, in \u001B[0;36mFacetGrid.map_dataframe\u001B[1;34m(self, func, *args, **kwargs)\u001B[0m\n\u001B[0;32m    816\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m data_ijk\n\u001B[0;32m    818\u001B[0m     \u001B[38;5;66;03m# Draw the plot\u001B[39;00m\n\u001B[1;32m--> 819\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_facet_plot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43max\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    821\u001B[0m \u001B[38;5;66;03m# For axis labels, prefer to use positional args for backcompat\u001B[39;00m\n\u001B[0;32m    822\u001B[0m \u001B[38;5;66;03m# but also extract the x/y kwargs and use if no corresponding arg\u001B[39;00m\n\u001B[0;32m    823\u001B[0m axis_labels \u001B[38;5;241m=\u001B[39m [kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m), kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PytorchL\\lib\\site-packages\\seaborn\\axisgrid.py:848\u001B[0m, in \u001B[0;36mFacetGrid._facet_plot\u001B[1;34m(self, func, ax, plot_args, plot_kwargs)\u001B[0m\n\u001B[0;32m    846\u001B[0m     plot_args \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    847\u001B[0m     plot_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124max\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m ax\n\u001B[1;32m--> 848\u001B[0m func(\u001B[38;5;241m*\u001B[39mplot_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mplot_kwargs)\n\u001B[0;32m    850\u001B[0m \u001B[38;5;66;03m# Sort out the supporting information\u001B[39;00m\n\u001B[0;32m    851\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_legend_data(ax)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PytorchL\\lib\\site-packages\\seaborn\\relational.py:645\u001B[0m, in \u001B[0;36mlineplot\u001B[1;34m(data, x, y, hue, size, style, units, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)\u001B[0m\n\u001B[0;32m    642\u001B[0m color \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolor\u001B[39m\u001B[38;5;124m\"\u001B[39m, kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    643\u001B[0m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolor\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m _default_color(ax\u001B[38;5;241m.\u001B[39mplot, hue, color, kwargs)\n\u001B[1;32m--> 645\u001B[0m \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43max\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    646\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ax\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PytorchL\\lib\\site-packages\\seaborn\\relational.py:432\u001B[0m, in \u001B[0;36m_LinePlotter.plot\u001B[1;34m(self, ax, kws)\u001B[0m\n\u001B[0;32m    427\u001B[0m     sort_cols \u001B[38;5;241m=\u001B[39m [var \u001B[38;5;28;01mfor\u001B[39;00m var \u001B[38;5;129;01min\u001B[39;00m sort_vars \u001B[38;5;28;01mif\u001B[39;00m var \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvariables]\n\u001B[0;32m    428\u001B[0m     sub_data \u001B[38;5;241m=\u001B[39m sub_data\u001B[38;5;241m.\u001B[39msort_values(sort_cols)\n\u001B[0;32m    430\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    431\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 432\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[43msub_data\u001B[49m\u001B[43m[\u001B[49m\u001B[43morient\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mvalue_counts()\u001B[38;5;241m.\u001B[39mmax() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    433\u001B[0m ):\n\u001B[0;32m    434\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munits\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvariables:\n\u001B[0;32m    435\u001B[0m         \u001B[38;5;66;03m# TODO eventually relax this constraint\u001B[39;00m\n\u001B[0;32m    436\u001B[0m         err \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mestimator must be None when specifying units\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PytorchL\\lib\\site-packages\\pandas\\core\\frame.py:3024\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3022\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3023\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3024\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3025\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3026\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PytorchL\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3082\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3080\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[0;32m   3081\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m-> 3082\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3084\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tolerance \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3085\u001B[0m     tolerance \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_tolerance(tolerance, np\u001B[38;5;241m.\u001B[39masarray(key))\n",
      "\u001B[1;31mKeyError\u001B[0m: 'x'"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 500x500 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = NormalizedActions(gym.make('Pendulum-v1'))\n",
    "agent = DDPG(device, env.action_space.shape[0], env.observation_space.shape[0], batch_size, gamma, tau)\n",
    "train_res = train(env,agent,train_eps,max_steps)\n",
    "test_res = test(env,agent,test_eps,max_steps)\n",
    "\n",
    "\n",
    "\n",
    "# 画出结果\n",
    "draw(train_res,tag=\"train\")\n",
    "draw(test_res,tag=\"test\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
