{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-11T16:19:31.340526600Z",
     "start_time": "2023-06-11T16:19:31.335136700Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import Env_1\n",
    "import Env_2\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class Actor(nn.Module):  #定义Actor网络\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(num_states, 128)\n",
    "        self.linear2 = nn.Linear(128, 128)\n",
    "        self.linear3 = nn.Linear(128, num_actions)\n",
    "        torch.nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear3.weight)\n",
    "\n",
    "    def change(self, x):\n",
    "        y = torch.zeros(x.shape).to(device)\n",
    "        if len(x.shape) == 1:\n",
    "            y[0] = x[0]*17.25 - 6.75\n",
    "            y[1] = x[1]*25.0\n",
    "            y[2] = x[2]*30\n",
    "            y[3] = torch.abs(x[3])\n",
    "        else :\n",
    "            y[:,0] = x[:,0]*17.25 - 6.75\n",
    "            y[:,1] = x[:,1]*25.0\n",
    "            y[:,2] = x[:,2]*30\n",
    "            y[:,3] = torch.abs(x[:,3])\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.linear1(x))\n",
    "        x = F.leaky_relu(self.linear2(x))\n",
    "        x = torch.tanh(self.linear3(x))\n",
    "        x = self.change(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T16:19:31.379789100Z",
     "start_time": "2023-06-11T16:19:31.342528900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class Critic(nn.Module):  #定义Critic网络\n",
    "    def __init__(self, num_state_action, num_action_value=1):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = nn.Linear(num_state_action, 128)\n",
    "        self.linear2 = nn.Linear(128, 128)\n",
    "        self.linear3 = nn.Linear(128, num_action_value)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear3.weight)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # 按维数1拼接\n",
    "\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.leaky_relu(self.linear1(x))\n",
    "        x = F.leaky_relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T16:19:31.379789100Z",
     "start_time": "2023-06-11T16:19:31.356128100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# a = Actor(14,4).to(torch.device('cuda:0'))\n",
    "# b = Critic(18,1).to(torch.device('cuda:0'))\n",
    "# state = torch.randn(256,14).to(torch.device('cuda:0'))\n",
    "# actor_loss = -b(state,a(state)).mean()\n",
    "#\n",
    "# print(actor_loss)\n",
    "# # print(a(state))\n",
    "# # print(next(a.parameters()), 'actor')\n",
    "# x1 = next(a.parameters()).clone()\n",
    "# optimizer = optim.Adam(a.parameters(), lr=0.001)\n",
    "# optimizer.zero_grad()\n",
    "# actor_loss.backward()\n",
    "# optimizer.step()\n",
    "# x2 = next(a.parameters()).clone()\n",
    "# print(torch.equal(x1.data, x2.data))\n",
    "# # print(next(a.parameters()), 'actor')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T16:19:31.388286600Z",
     "start_time": "2023-06-11T16:19:31.369246100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=self.capacity)\n",
    "\n",
    "    def push(self, transitions):\n",
    "        '''_summary_\n",
    "        Args:\n",
    "            trainsitions (tuple): _description_\n",
    "        '''\n",
    "        self.buffer.append(transitions)\n",
    "\n",
    "    def sample(self, batch_size: int, sequential: bool = False):\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "        if sequential:  # sequential sampling\n",
    "            rand = random.randint(0, len(self.buffer) - batch_size)\n",
    "            batch = [self.buffer[i] for i in range(rand, rand + batch_size)]\n",
    "            return zip(*batch)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "            return zip(*batch)\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T16:19:31.399807Z",
     "start_time": "2023-06-11T16:19:31.388286600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def clip(x):\n",
    "        y = torch.zeros(x.shape).to(device)\n",
    "        if len(x.shape) == 1:\n",
    "            y[0] = torch.clip(x[0],-24.0,10.5)\n",
    "            y[1] = torch.clip(x[1],-25.0,25.0)\n",
    "            y[2] = torch.clip(x[2],-30.0,30.0)\n",
    "            y[3] = torch.clip(x[3], 0.0,1.0)\n",
    "        else :\n",
    "            y[:,0] = torch.clip(x[:,0],-24.0,10.5)\n",
    "            y[:,1] = torch.clip(x[:,1],-25.0,25.0)\n",
    "            y[:,2] = torch.clip(x[:,2],-30.0,30.0)\n",
    "            y[:,3] = torch.clip(x[:,3], 0.0,1.0)\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T16:19:31.428264600Z",
     "start_time": "2023-06-11T16:19:31.402889900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class TD3:\n",
    "    def __init__(self, device, action_space, state_space, batch_size, gamma, tau, action_noise,policy_noise,policy_noise_clip,delay_time):\n",
    "        self.device = device\n",
    "        self.critic1 = Critic(action_space + state_space, 1).to(device)\n",
    "        self.critic2 = Critic(action_space + state_space, 1).to(device)\n",
    "        self.actor = Actor(state_space, action_space).to(device)\n",
    "        self.action_noise = action_noise\n",
    "        self.policy_noise = policy_noise\n",
    "        self.policy_noise_clip = policy_noise_clip\n",
    "        self.delay_time = delay_time\n",
    "        self.update_time = 0\n",
    "\n",
    "\n",
    "        if os.path.exists('actor_dict'):\n",
    "            self.actor.load_state_dict(torch.load('actor_dict'))\n",
    "\n",
    "        if os.path.exists('critic1_dict'):\n",
    "            self.critic1.load_state_dict(torch.load('critic1_dict'))\n",
    "\n",
    "        if os.path.exists('critic2_dict'):\n",
    "            self.critic2.load_state_dict(torch.load('critic2_dict'))\n",
    "\n",
    "        self.target_critic1 = Critic(action_space + state_space, 1).to(device)\n",
    "        self.target_critic2 = Critic(action_space + state_space, 1).to(device)\n",
    "        self.target_actor = Actor(state_space, action_space).to(device)\n",
    "\n",
    "        self.update_network_parameters(tau=1.0)\n",
    "\n",
    "        # self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=5e-3)\n",
    "        # self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=5e-3)\n",
    "        # self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "        self.memory = ReplayBuffer(capacity=100000)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        for actor_params, target_actor_params in zip(self.actor.parameters(),\n",
    "                                                     self.target_actor.parameters()):\n",
    "            target_actor_params.data.copy_(tau * actor_params + (1 - tau) * target_actor_params)\n",
    "\n",
    "        for critic1_params, target_critic1_params in zip(self.critic1.parameters(),\n",
    "                                                         self.target_critic1.parameters()):\n",
    "            target_critic1_params.data.copy_(tau * critic1_params + (1 - tau) * target_critic1_params)\n",
    "\n",
    "        for critic2_params, target_critic2_params in zip(self.critic2.parameters(),\n",
    "                                                         self.target_critic2.parameters()):\n",
    "            target_critic2_params.data.copy_(tau * critic2_params + (1 - tau) * target_critic2_params)\n",
    "\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        action = self.actor(state)\n",
    "        noise = torch.tensor(np.random.normal(loc=0.0, scale=self.action_noise),\n",
    "                             dtype=torch.float).to(self.device)\n",
    "        action = clip(action+noise)\n",
    "        return action.detach().cpu().numpy()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_action(self, state):\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        action = self.target_actor(state)\n",
    "        return action.cpu().numpy()\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.memory) < self.batch_size:  # 当memory中不满足一个批量时，不更新策略\n",
    "            # print('no')\n",
    "            return\n",
    "\n",
    "        critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=5e-3)\n",
    "        critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=5e-3)\n",
    "        actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "\n",
    "        # 从经验回放中中随机采样一个批量的transition\n",
    "        state, action, reward, next_state, done = self.memory.sample(self.batch_size)\n",
    "        # 转变为张量\n",
    "\n",
    "        state = torch.FloatTensor(np.array(state)).to(self.device)\n",
    "        next_state = torch.FloatTensor(np.array(next_state)).to(self.device)\n",
    "        action = torch.FloatTensor(np.array(action)).to(self.device)\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1).to(self.device)\n",
    "        done = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # print(state,'state',next_state,'next_state',action,'action',reward,'reward',done,'done')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_action = self.target_actor(next_state)\n",
    "\n",
    "            action_noise = torch.tensor(np.random.normal(loc=0.0, scale=self.policy_noise),dtype=torch.float).to(self.device)\n",
    "                # smooth noise\n",
    "            action_noise = torch.clamp(action_noise, -self.policy_noise_clip, self.policy_noise_clip)\n",
    "\n",
    "            next_action = clip(next_action + action_noise)\n",
    "\n",
    "            q1_ = self.target_critic1(next_state, next_action)\n",
    "            q2_ = self.target_critic2(next_state, next_action)\n",
    "            q_ = torch.min(q1_, q2_)\n",
    "            target_value = reward + (1.0 - done) * self.gamma * q_\n",
    "\n",
    "\n",
    "        q1 = self.critic1(state, action)\n",
    "        q2 = self.critic2(state, action)\n",
    "\n",
    "        critic1_loss = nn.MSELoss()(q1, target_value.detach())\n",
    "        critic2_loss = nn.MSELoss()(q2, target_value.detach())\n",
    "\n",
    "        # graph=make_dot(critic1_loss,params=dict(self.critic1.named_parameters()),)  # 生成计算图结构表示\n",
    "        # graph.render(filename='critic1_loss',view=False,format='png')  # 将源码写入文件，并对图结构进行渲染\n",
    "\n",
    "        # print(self.critic_loss, 'critic_loss')\n",
    "\n",
    "        # self.critic1_optimizer.zero_grad()\n",
    "        # critic1_loss.backward()\n",
    "        # self.critic1_optimizer.step()\n",
    "        # self.critic2_optimizer.zero_grad()\n",
    "        # critic2_loss.backward()\n",
    "        # self.critic2_optimizer.step()\n",
    "\n",
    "        critic1_optimizer.zero_grad()\n",
    "        critic1_loss.backward()\n",
    "        critic1_optimizer.step()\n",
    "\n",
    "        critic2_optimizer.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        critic2_optimizer.step()\n",
    "\n",
    "        self.update_time += 1\n",
    "        if self.update_time % self.delay_time != 0:\n",
    "            return\n",
    "\n",
    "        q1 = self.critic1(state, self.actor(state))\n",
    "        actor_loss = -q1.mean()\n",
    "\n",
    "        # print(actor_loss, 'actor_loss')\n",
    "\n",
    "        # for param in self.actor.parameters():\n",
    "        #     param.requires_grad = True\n",
    "        # graph=make_dot(self.actor_loss,params=dict(self.actor.named_parameters()),)  # 生成计算图结构表示\n",
    "        # graph.render(filename='actor_loss',view=False,format='png')  # 将源码写入文件，并对图结构进行渲染\n",
    "\n",
    "        # self.actor_optimizer.zero_grad()\n",
    "        # actor_loss.backward()\n",
    "        # self.actor_optimizer.step()\n",
    "\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        self.update_network_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T16:19:31.445744100Z",
     "start_time": "2023-06-11T16:19:31.415090800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def test(env, agent, test_eps, max_steps):\n",
    "    print(\"Start Testing\")\n",
    "    rewards = []  # 记录所有回合的奖励\n",
    "\n",
    "    for episode in range(test_eps):\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 5))\n",
    "        ax1 = Axes3D(fig)\n",
    "        ax1.plot3D([0], [0], [0], 'red')\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        z = []\n",
    "        for step in range(max_steps):\n",
    "            action = agent.predict_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            x.append(env.x_g)\n",
    "            y.append(env.y_g)\n",
    "            z.append(-env.z_g)\n",
    "\n",
    "            ax1.plot3D(x, y, z, 'red')\n",
    "            display.display(fig)\n",
    "            plt.pause(0.001)\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        # plt.pause(10)\n",
    "        rewards.append(ep_reward)\n",
    "        print(f\"Episode：{episode + 1}/{test_eps}，Reward：{ep_reward:.2f}\")\n",
    "    print(\"Testing Complete\")\n",
    "    return rewards"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T16:19:31.497229Z",
     "start_time": "2023-06-11T16:19:31.445744100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def train(env, agent, train_eps, test_eps, max_steps):\n",
    "    print(\"Start Training\")\n",
    "    rewards = []  # 记录所有回合的奖励\n",
    "    great_performance = 0\n",
    "\n",
    "    for episode in range(train_eps):\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "\n",
    "        if episode > 499 and episode % 500 == 0:\n",
    "            draw = 1\n",
    "        else:\n",
    "            draw = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "\n",
    "            action = agent.sample_action(state)\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            ep_reward += reward\n",
    "\n",
    "            agent.memory.push((state, action, reward, next_state, done))\n",
    "            agent.update()\n",
    "            state = next_state\n",
    "\n",
    "            # display.clear_output(wait=True)\n",
    "\n",
    "            if draw :\n",
    "                testres = test(env, agent, test_eps, max_steps)\n",
    "                fig = plt.plot(testres)\n",
    "                display.display(fig)\n",
    "\n",
    "            if done:\n",
    "                # display.clear_output(wait=True)\n",
    "                break\n",
    "\n",
    "        # print(ep_reward)\n",
    "        if great_performance < ep_reward and episode > 500:\n",
    "                great_performance = ep_reward\n",
    "                torch.save(agent.target_critic1.state_dict(), 'critic1_dict')\n",
    "                torch.save(agent.target_critic2.state_dict(), 'critic2_dict')\n",
    "                torch.save(agent.target_actor.state_dict(), 'actor_dict')\n",
    "                print('save')\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            # print(next(agent.actor.parameters()), episode, 'actor')\n",
    "            # print(next(agent.critic1.parameters()), episode, 'critic1')\n",
    "            # x111 = next(agent.actor.parameters())\n",
    "            # print(torch.equal(x111.data, x222.data))\n",
    "            # display.clear_output(wait=True)\n",
    "\n",
    "            print(f\"Episode：{episode + 1}/{train_eps}，Reward：{ep_reward:.2f}\")\n",
    "            # display.clear_output(wait=True)\n",
    "        rewards.append(ep_reward)\n",
    "    print(\"Training Complete\")\n",
    "    return rewards"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T16:19:31.497229Z",
     "start_time": "2023-06-11T16:19:31.467422300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# env = gym.make('Pendulum-v1')\n",
    "# env.action_space.shape[0], env.observation_space.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T16:19:31.497229Z",
     "start_time": "2023-06-11T16:19:31.479163100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "env = Env_2.Plane(0,100,65,40,0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T16:19:31.538982800Z",
     "start_time": "2023-06-11T16:19:31.495227600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# 超参数\n",
    "train_eps = 6000000\n",
    "test_eps = 1\n",
    "max_steps = 500000\n",
    "gamma = 0.99\n",
    "batch_size = 512\n",
    "device = torch.device('cuda:0')\n",
    "tau = 0.001\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T16:19:31.579283600Z",
     "start_time": "2023-06-11T16:19:31.510182300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Episode：10/6000000，Reward：1061.47\n",
      "Episode：20/6000000，Reward：582.49\n",
      "Episode：30/6000000，Reward：1279.01\n",
      "Episode：40/6000000，Reward：477.76\n",
      "Episode：50/6000000，Reward：1242.95\n",
      "Episode：60/6000000，Reward：356.83\n",
      "Episode：70/6000000，Reward：263.86\n",
      "Episode：80/6000000，Reward：312.91\n",
      "Episode：90/6000000，Reward：347.78\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m agent \u001B[38;5;241m=\u001B[39m TD3(device, env\u001B[38;5;241m.\u001B[39maction_space, env\u001B[38;5;241m.\u001B[39mobservation_space, batch_size \u001B[38;5;241m=\u001B[39m batch_size, gamma\u001B[38;5;241m=\u001B[39m gamma, tau\u001B[38;5;241m=\u001B[39m tau, action_noise\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.75\u001B[39m, policy_noise\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m, policy_noise_clip\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,delay_time\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m train_res \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_eps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_eps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(agent\u001B[38;5;241m.\u001B[39mtarget_critic1\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcritic1_dict\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      4\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(agent\u001B[38;5;241m.\u001B[39mtarget_critic2\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcritic2_dict\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[22], line 24\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(env, agent, train_eps, test_eps, max_steps)\u001B[0m\n\u001B[0;32m     21\u001B[0m ep_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n\u001B[0;32m     23\u001B[0m agent\u001B[38;5;241m.\u001B[39mmemory\u001B[38;5;241m.\u001B[39mpush((state, action, reward, next_state, done))\n\u001B[1;32m---> 24\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m state \u001B[38;5;241m=\u001B[39m next_state\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# display.clear_output(wait=True)\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[20], line 82\u001B[0m, in \u001B[0;36mTD3.update\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;66;03m# 转变为张量\u001B[39;00m\n\u001B[0;32m     81\u001B[0m state \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mFloatTensor(np\u001B[38;5;241m.\u001B[39marray(state))\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m---> 82\u001B[0m next_state \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFloatTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_state\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m     83\u001B[0m action \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mFloatTensor(np\u001B[38;5;241m.\u001B[39marray(action))\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m     84\u001B[0m reward \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mFloatTensor(reward)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "agent = TD3(device, env.action_space, env.observation_space, batch_size = batch_size, gamma= gamma, tau= tau, action_noise=0.75, policy_noise=0.5, policy_noise_clip=1,delay_time=3)\n",
    "train_res = train(env, agent, train_eps, test_eps, max_steps)\n",
    "torch.save(agent.target_critic1.state_dict(), 'critic1_dict')\n",
    "torch.save(agent.target_critic2.state_dict(), 'critic2_dict')\n",
    "torch.save(agent.target_actor.state_dict(), 'actor_dict')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T16:19:59.705103200Z",
     "start_time": "2023-06-11T16:19:31.527211700Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
